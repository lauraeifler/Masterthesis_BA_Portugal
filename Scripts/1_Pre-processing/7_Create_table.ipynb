{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb17d4c-eff4-4c23-8533-5ef6064a9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.transform import rowcol\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab62e97-4f80-41cc-a3e3-3c2496dbe066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the data\n",
    "path_ba = 'path-to-final-BA-data'\n",
    "path_pa = \"path-to-WDPA-netCDF-file\" \n",
    "path_lc = 'path-to-final-LC-data'\n",
    "path_clim = 'path-to-final-FWI-files'\n",
    "\n",
    "## output\n",
    "output_table = 'path-and-name-of-final-table'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519bf54f-8d0c-4907-9254-7c4cf42f2fc5",
   "metadata": {},
   "source": [
    "## Sample BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad029c5-c989-408d-8fd9-d45cea5de51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecct BA pixels looping through all years\n",
    "data = []\n",
    "# case_limit_per_year = 2  # Limit to 1000 pixels per year\n",
    "random.seed(42)\n",
    "\n",
    "# Loop through each subfolder (e.g., 2001 to 2020)\n",
    "for year_folder in sorted(os.listdir(path_ba)):  # Ensure consistent ordering\n",
    "    year_path = os.path.join(path_ba, year_folder)\n",
    "    if not os.path.isdir(year_path):\n",
    "        continue  # Skip files or irrelevant content\n",
    "\n",
    "    # Storage for all pixels in this year\n",
    "    year_data = []\n",
    "\n",
    "    # Loop through each file in the subfolder\n",
    "    for file_name in sorted(os.listdir(year_path)):  # Sort for consistency\n",
    "        if not file_name.endswith(\".tif\"):\n",
    "            continue  # Skip non-raster files\n",
    "\n",
    "        # Extract year and month from the file name\n",
    "        year = file_name[:4]  # Assumes the year is the first 4 characters\n",
    "        month = file_name[4:6]  # Assumes the month is the next 2 characters\n",
    "\n",
    "        # Open the raster file\n",
    "        file_path = os.path.join(year_path, file_name)\n",
    "        with rasterio.open(file_path) as src:\n",
    "            # Read the data and the transform (geolocation information)\n",
    "            data_array = src.read(1)  # Assuming data is in the first band\n",
    "            transform = src.transform\n",
    "\n",
    "            # Loop through the array to find non-zero (burned area) pixels\n",
    "            rows, cols = data_array.shape\n",
    "            for row in range(rows):\n",
    "                for col in range(cols):\n",
    "                    burned_value = data_array[row, col]\n",
    "                    if burned_value > 0:  # Check for burned area\n",
    "                        # Convert array indices to lat/lon\n",
    "                        lon, lat = rasterio.transform.xy(transform, row, col, offset='center')\n",
    "\n",
    "                        # Append data to the year's list\n",
    "                        year_data.append({\n",
    "                            \"Year\": int(year),\n",
    "                            \"Month\": int(month),\n",
    "                            \"Latitude\": lat,\n",
    "                            \"Longitude\": lon,\n",
    "                            \"Burned_Area\": burned_value\n",
    "                        })\n",
    "\n",
    "    # Add the selected data to the main list\n",
    "    data.extend(year_data)\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31661276-2f10-4ddf-805c-95feba9c976d",
   "metadata": {},
   "source": [
    "## Add PA information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d3ff0-ab39-4d6c-a92b-5402354e736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Copy df\n",
    "df_pa = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb6d660-0c00-4654-a9e1-0a2c1ffe8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each burned pixel in the DataFrame\n",
    "PA_values = []  # To store PA values for each burned pixel\n",
    "PA_year_values = []\n",
    "\n",
    "\n",
    "for _, row in df_pa.iterrows():\n",
    "    year = int(row['Year'])\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    \n",
    "    \n",
    "    # Construct the filename for the previous year\n",
    "    input_filename = f\"name-of-WDPA-netCDF-file\"\n",
    "    input_file = os.path.join(path_pa, input_filename)\n",
    "    \n",
    "    \n",
    "    # Open the dataset for the previous year\n",
    "    pa_data = xr.open_dataset(input_file)\n",
    "    \n",
    "    \n",
    "    # Extract the latitude and longitude indices\n",
    "    lat_idx = np.abs(pa_data['Latitude'].values - lat).argmin()\n",
    "    lon_idx = np.abs(pa_data['Longitude'].values - lon).argmin()\n",
    "    \n",
    "    # Extract the PA info for the corresponding pixel (previous year)\n",
    "    pa_status = pa_data['PA_status'].isel(Latitude=lat_idx, Longitude=lon_idx).values\n",
    "    pa_year = pa_data['PA_year'].isel(Latitude=lat_idx, Longitude=lon_idx).values\n",
    "    \n",
    "    # Append the PA value to the list\n",
    "    PA_values.append(pa_status)\n",
    "    PA_year_values.append(pa_year)\n",
    "\n",
    "# Add the values to the original dataframe as a new column\n",
    "df_pa['PA_status'] = PA_values\n",
    "df_pa['PA_year'] = PA_year_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37289f55-a5e8-4b97-968e-9bd2e934364c",
   "metadata": {},
   "source": [
    "## Add LC information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a845bf-95bf-44d6-8425-91fc5a450733",
   "metadata": {},
   "outputs": [],
   "source": [
    "### copy dataframe\n",
    "df_lc = df_pa.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbaa8b9-d6b3-4ab4-8150-78cca5afc5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LC class maximum value\n",
    "# Loop through each burned pixel in the DataFrame\n",
    "lccs_values = []  # To store lccs values for each burned pixel\n",
    "\n",
    "for _, row in df_lc.iterrows():\n",
    "    year = int(row['Year'])\n",
    "    lat = row['Latitude']\n",
    "    lon = row['Longitude']\n",
    "    \n",
    "    # Get the previous year for the LC data\n",
    "    previous_year = year - 1\n",
    "    \n",
    "    # Determine the version based on the year\n",
    "    if previous_year <= 2015:\n",
    "        file_version = 'v2.0.7'\n",
    "    else:\n",
    "        file_version = 'v2.1.1'\n",
    "    \n",
    "    # Construct the filename for the previous year\n",
    "    input_filename = f\"ESACCI-LC-L4-LCCS-Map-300m-P1Y-{previous_year}-{file_version}_agg_1km.nc\"\n",
    "    input_file = os.path.join(path_lc, input_filename)\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"File for {previous_year} not found: {input_file}\")\n",
    "        lccs_values.append(np.nan)  # Append NaN if file not found\n",
    "        continue\n",
    "    \n",
    "    # Open the dataset for the previous year\n",
    "    lc_data = xr.open_dataset(input_file)\n",
    "    \n",
    "    # Check if 'lccs_class_new' exists in the dataset\n",
    "    if 'lccs_class_new' not in lc_data:\n",
    "        print(f\"Variable 'lccs_class_new' not found in {input_file}\")\n",
    "        lccs_values.append(np.nan)  # Append NaN if variable not found\n",
    "        continue\n",
    "    \n",
    "    # Extract the latitude and longitude indices\n",
    "    lat_idx = np.abs(lc_data['lat'].values - lat).argmin()\n",
    "    lon_idx = np.abs(lc_data['lon'].values - lon).argmin()\n",
    "    \n",
    "    # Extract the LC class for the corresponding pixel (previous year)\n",
    "    lc_class = lc_data['lccs_class_new'].isel(time=0, lat=lat_idx, lon=lon_idx).values\n",
    "    \n",
    "    # Append the LC class value to the list\n",
    "    lccs_values.append(lc_class)\n",
    "\n",
    "# Add the lccs_class_new values to the original dataframe as a new column\n",
    "df_lc['lccs'] = lccs_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef59cc9-8c77-4fe2-a1b5-a8d59557d5c7",
   "metadata": {},
   "source": [
    "## Add FWI max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fef100-0f93-486f-8067-98c0d4a48ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### copy dataframe\n",
    "df_all = df_lc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134f925b-093e-435e-83fa-0b99533e9bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add temperature statistics to the DataFrame\n",
    "def add_FWI(df, clim_data_path):\n",
    "    # Iterate over unique years in the burned area data\n",
    "    for year in df['Year'].unique():\n",
    "        # Open the NetCDF file for the corresponding year\n",
    "        clim_file = os.path.join(clim_data_path, f\"ERA5_Land_FWI_1km_{year}.nc\")  # Example naming pattern\n",
    "        if not os.path.exists(clim_file):\n",
    "            print(f\"Data for year {year} not found!\")\n",
    "            continue\n",
    "\n",
    "        with xr.open_dataset(clim_file) as ds:\n",
    "            # Assume the dataset has variables 'temperature', 'lat', 'lon', and 'time'\n",
    "            fwi = ds['fwi']\n",
    "            time = pd.to_datetime(ds['valid_time'].values)\n",
    "\n",
    "            # Loop through each row in the DataFrame for this year\n",
    "            for i, row in df[df['Year'] == year].iterrows():\n",
    "                month = int(row['Month'])  # Burned area month\n",
    "                target_lat, target_lon = row['Latitude'], row['Longitude']\n",
    "\n",
    "                \n",
    "                # FWI\n",
    "                nearest_fwi = fwi.sel(\n",
    "                    latitude=target_lat, longitude=target_lon, method=\"nearest\"\n",
    "                )\n",
    "\n",
    "\n",
    "                # Filter by the month\n",
    "                month_days = (time.month == month)\n",
    "                monthly_fwi = nearest_fwi[month_days].values\n",
    "\n",
    "                # Calculate statistics if there is valid data\n",
    "                if monthly_fwi.size > 0:\n",
    "                    df.loc[i, 'fwi_mean'] = np.mean(monthly_fwi)\n",
    "                    df.loc[i, 'fwi_max'] = np.max(monthly_fwi)\n",
    "                else:\n",
    "                    # Fill with NaN if no data is available\n",
    "                    df.loc[i, ['fwi_mean', 'fwi_max']] = [np.nan] * 4\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return df\n",
    "    \n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df_all = add_FWI(df_all, path_clim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31769be1-ab4c-49dc-8e29-09d7ce38ada5",
   "metadata": {},
   "source": [
    "## Filter and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7499a98-b152-4184-b540-874c506c6fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter the data\n",
    "df_all[\"PA_year\"] = df_all[\"PA_year\"].fillna(0)\n",
    "df_all[\"PA_year\"] = df_all[\"PA_year\"].astype(int)\n",
    "df_all[\"Year\"] = df_all[\"Year\"].astype(int)\n",
    "df_all[\"Month\"] = df_all[\"Month\"].astype(int)\n",
    "df_all[\"lccs\"] = df_all[\"lccs\"].astype(int)\n",
    "# Filter rows where lcc is 1, 2, or 3 (only forest)\n",
    "# df_filtered = df_all[df_all[\"lcc\"].isin([1, 2, 3])]\n",
    "\n",
    "# Remove rows where Year (year of fire) - PA_year is negative (retain rows with NaN in PA_year) - Protection before the fire started is kept\n",
    "df_filtered = df_all[(df_all[\"Year\"] - df_all[\"PA_year\"]) >= 0]\n",
    "\n",
    "# Filter rows where lcc is 1, 2, or 3 (only forest)\n",
    "df_forest = df_forest[df_forest[\"lccs\"].isin([1, 2, 3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a11048a-1a78-4dcc-8c51-a1ee441b23eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10910e-856d-4235-953b-a443b77699f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4647f510-babd-43ac-bcd4-64aaf402f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save as csv table\n",
    "df_forest.to_csv(output_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e15e8a-32e5-4086-b48b-4577c473711e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MasterEnv",
   "language": "python",
   "name": "master_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
